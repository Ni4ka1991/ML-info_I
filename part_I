КОНСПЕКТ

*************
Общая информация

Понятие развития нейронов связано с понятием пластичности мозга -- способности настройки нервной системы в соответствии с окружающими условиями.
Именно пластичность играет самую важную роль в работе нейронов в качестве единиц обработки информации в человеческом мозге.
Аналогично, в искусственных нейронных сетях работа проводится с искусственными нейронами.

*В общем случае нейронная сеть представляет собой машину, моделирующую способ обработки мозгом конкретной задачи.
Эта сеть обычно реализуется с помощью электронных компонентов или моделируется программой, выполняемой на цифровом компьютере.*

В литературе часто нейронные сети называют:
    - нейрокомпьютерами
    - сетями связей
    - параллельными распределенными поцессорами
    - ...

*************
1. Искусственный нейрон (ИН)

1.1.  Принцип организации биологического нейрона (БН) совсем не похож на базовые модели, которые сейчас используются в ИИ и НС.
          - все нейроны разные
          - БН сам по себе работать не может (гипердинамическая система - нет энергии нет сущестования). Запасы на 6 минут.
          (Заметка: колибри если не пьет нектар для питания мозга (Колибри(мозг/массе теле) > Человек(мозг/массе теле)), впадает в торпидность во время которой почти все
          забывает)
          - на одном БН синапсов 100 тыс - 1 мил 
               
                I I I
                V V V 
                
        МАТЕМАТИЧЕСКОЕ УПРОЩЕНИЕ
        
                I I I
                V V V
        

1.2. Базовая модель искусственного нейрона, используемая в НС: 

Трехслойный компонент (существует во всех математических моделях до сих пор): вектор входных данных (очень сложно оценить) -> передаточная фонкция -> результат на выходе

ЭКСКУРС В ИСТОРИЮ ---> 

Упрощенная модель БН  (развитие представлений о БН):

Модель Лапика 1907 год:
    Нейрон получает сигнал от своих соседей, накапливает заряд (суммирует информацию), заряд разряжается (выдает результирующий сигнал).
        - вход нейронов, как поток ионов через каналы, регулируемые трансмитерами;
        - клеточная мембрана, как конденсатор переменной ёмкости;
        - разность потенциалов, изменение напряжения и потенциал действия - выход нейронов;
    (слабое место модели -> Бесконечное повышение напряжения )

Модель МакКаллока_Питтса 1943 (ключевая математическая модель БН в нейрокомпьютерах и нейромоделировании до сих пор): / по сути пороговый линейный классификатор /
        - нейрон ничего не изменяет при передаче сигнала;
        - дополнена принципом порога: "все или ничего", 1 vs 0
        - дополнена вводными сигналами с отрицательными весами (тормозные синапсы)
        - память - один нейрон с петлей связи на самого себя

Модель Ходжкина-Хаксли /постоена на аксоне кальмара/ (1952), упрощенная Хитцхью-Нагумо (1962):
        - регенеративное самовозбуждение
        - нелинейная связь
        - торможение
           I
           V
       возможность моделировать такое поведение с помощью компьютеров


1.3. Математическая модель искусственного нейрона
     Математически искусственный нейрон обычно представляют как некоторую нелинейную функцию от единственного аргумента – линейной комбинации всех входных сигналов.
     Эту функцию называют функцией активации, функцией срабатывания или передаточной функцией нейрона. Полученный результат отправляется на единственный выход нейрона.

x1    -> w1        ->  x1*w1  -------->  \    
x2    -> w2        ->  x2*w2  ------->    \  
...                                         сумма(∑ ) ---> применяем нелинейную функцию к этой сумме (f)  --->  добавляем ЧТО-ТО1  ----->  иногда добавляем  ЧТО-ТО2     ---->      получаем Y
x(n) -> w(n)       ->  x(n)*w(n) ----- ->  /               
                                                                      I                                                   I                         I                                                              
                                                                      V                                                   V                         V                    
                                                                функция активации                   формиров. порога чувствительности (θ)   случайная велечина (смещение)


y = f(∑XiWi + θ ) 

Простейшая модель: x принимает значения х[0 или 1]. Ноль означает, что эта связь не активированна. Соответствено OUT[0 или 1].
Персептрон устроен таким образом, что его внутренняя "жизнь" описывается весовыми коэффициентами: w∈ℝ -> (w являются элементами множества всех конечных и бесконечных десятичных дробей-действительных чисел) . 
ИН действует дискретно. Предполагается, что мы подаем на вход [x1-xn], проходит какое-то время и он выдает на выходе соответствующий OUT. Выход OUT определяется видом функции активации
и может быть, как действительным OUT∈ℝ, так и целым OUT∈ℤ. Все ВХОДЫ устоявшиеся.

Все входы xi умножаются на значения своих весов wi, суммируются и дополняются некоторым значением θ . Затем сумма при помощи функции активации передается на выход нейрона.
(Заметка: в естественных нейронах есть пороговый потенциал, когда он достигается, включается аксон и сигнал передается другим нейронам. 
Поведение включения эмулируется активационной функцией. Активационная функция добавляет нелинейность обработки поступающих сигналов, потому что БН имеет нелинейное передаточное поведение.
Активационная функция в некоторых случаях может быть линейной.)

В нейронных сетях, синапсы представляют собой соединения (как? через что?) между нейронами и имеют возможность усиливать или смягчать нейронные сигналы.
Нейронные весы (weights) могут повлиять на нейронный вывод (output), следовательно нейронная активация зависима от ввода и от весов.
При условии, что inputs идут от других нейронов или от внешнего мира, весы (weights) считаются установленными нейронными соединениями между нейронами.
Таким образом, весы являются внутренними для нейронных сетей, мы можем считать их как знания нейронных сетей, предоставленные изменения весов будут изменять возможности нейронных сетей и поэтому — действия.

Значение θ используется для инициализации нейрона (x0 = -1 из лекции Шамина? независимый элемент ИН). Под инициализацией имеется в виду смещение функции активации нейрона
по горизонтальной оси (не наглядно мне), т.е. формирование порога чувствительности нейрона. Пороговый элемент (bias). Эта величина отражает увеличение или уменьшение входного сигнала, 
подаваемого на функцию активации. Использование порога θ обеспечивает эффект афинного преобразования выхода линейного сумматора |ADD 2|.
Кроме того, иногда к выходу нейрона специально добавляют некоторую случайную величину, которая называется смещением.
Смещение можно рассматривать как сигнал на дополнительном, всегда нагруженном синапсе (синапсы – это связи, по которым выходные сигналы одних нейронов попадают на входы других).

1.4. Варианты реализации структурных составляющих ИН: 
        - К основным компонентам искусственного нейрона относятся:
            - умножители (синапсы) |MUL|
            - сумматор-аккумулятор |ADD|
            - блок функции активации нейрона - нелинейный преобразователь |FUNC| 

                 w1
                 I
                 V
 x1 ->       | MUL |                    \
                                         V
                 w2
                  I
                  V
 x2 ->       | MUL |            -->   |  ADD |                      
                                                \
                                                 V
                                                                        
 θ   ---------------------------------------->  | ADD2 |  -->  | FUNC|                    --->  OUT

                wn
                 I
                 V                      ^
 xn ->       | MUL |                   /

    - Умножители  | MUL |. В идеальном случае, количество умножителей должно быть равно количеству входов нейрона – только таким образом может быть достигнута максимальная параллельность вычислений. С другой стороны, поскольку приходится иметь дело с дробными числами (как с фиксированной, так и с плавающей запятой), реализовать абсолютно параллельную ИНС не представится возможным, поскольку             арифметические операции над дробными числами слишком ресурсоемки. Есть два пути решения этой проблемы: упрощение и оптимизация умножителей и уменьшение количества умножителей, например,                   за счет использования всеми входами нейрона одного умножителя

    - Сумматор-аккумулятор  |  ADD |. Сложность этого элемента так же определяется сложностью алгоритма обработки дробных чисел.

    - Функция активации. Функция активации определяет зависимость сигнала на выходе нейрона от взвешенной суммы сигналов на его входах. В большинстве случаев она является монотонно возрастающей и имеет область значений (-1, 1) или (0, 1). Бывает нескольких типов, причем в искусственных нейронных сетях чаще всего используется нелинейная функция активации - сигмоида*.
             (Заметка:  Аппаратная реализация нелинейной сигмоидальной функции хоть и возможна, но очень проблематична. Существует несколько способов упрощения аппаратной реализации сигмоидальной функции: использование поисковых таблиц (lookup table, по сути - память), различные виды кусочно-линейной аппроксимации, комбинационная аппроксимация.)

* По аналогии с электронными системами активационную функцию можно считать нелинейной усилительной характеристикой искусственного нейрона.
Коэффициент усиления вычисляется как отношение приращения величины OUT к вызвавшему его небольшому приращению величины ADD ( K = дельта OUT / дельта ADD ---> OUT = K*ADD).
Он выражается наклоном кривой при определенном уровне возбуждения и изменяется от малых значений прибольших отрицательных возбуждениях (кривая почти горизонтальна) до максимального значения при нулевом возбуждении и снова уменьшается, когда возбуждение становится большим положительным.
Гроссберг (1973) обнаружил, что подобная нелинейная характеристика решает поставленную им дилемму шумового насыщения.
Каким образом одна и та же сеть может обрабатывать как слабые, так исильные сигналы? Слабые сигналы нуждаются в большом сетевом усилении, чтобы дать пригодный к использованию выходной сигнал.
Однако усилительные каскады с большими коэффициентами усиления могут привести к насыщению выхода шумами усилителей (случайными флуктуациями), которые присутствуют в любой физически реализованной сети.
Сильные входные сигналы в свою очередь также будут приводить к насыщению усилительных каскадов, исключая возможность полезного использования выхода. 
Центральная область сигмоидальной функции, имеющая большой коэффициент усиления, решает проблему обработки слабых сигналов, в то время как области с падающим усилением на положительном и отрицательном концах подходят для больших возбуждений. 
Если я все правильно поняла, такая функция отлично усиливает слабые сигналы (ADD ~ около нуля) и слабо реагирует на сверх-сигналы .


Поскольку модели ИНС во многом зависят от массивных параллельных вычислений, чтобы обеспечить высокую скорость работы в режиме реального времени, НС должны быть реализованы с помощью применения параллельных аппаратных структур.

****************************************
2. Нейронная сеть

2.1. Нейронная сеть -- это громадный распределенный параллельный процессор, состоящий из элементарных единиц обработки информации, накапливающих экспериментальные знания и
предоставляющих их для последующей обработки.
Нейронная сеть сходна с мозгом с двух точек зрения.
    - Знания поступают в нейронную сеть из окружающей среды и используются в процессе обучения.
    - Для накопления знаний применяются связи между нейронами, называемые синаптическими весами. *

 Нейронные сети могут изменять свою собственную топологию.

2.2. Нейронные сети 
во-первых, распараллеливание обработки информации
во-вторых, способность самообучаться, т.е. создавать обобщения.
Под термином обобщение понимается способность получать обоснованный результат на основании данных, которые не встречались в процессе обучения.*

2.3. Искусственные нейронные сети (ИНС) - это математические модели, а также их программные или аппаратные реализации.

2.4. Сложность реализации любой ИНС зависит от сложности реализации отдельного нейрона (программного???).

2.5. ИС первоначально выполнялись как электронные сети, позднее были перенесены в более гибкую среду компьютерного моделирования, сохранившуюся и в настоящее время. 

2.6. Cети, состоящие из одного слоя искусственных нейронов часто называемые персептронами

2.7. Искусственные нейронные сети представляют собой систему соединенных (как? чем?) и взаимодействующих между собой простых процессоров (виртуальных???) (ИН).
Такие процессоры обычно довольно просты, особенно по сравнению с процессорами, которые используются в персональных компьютерах. Каждый процессор подобной сети имеет
дело только с сигналами, которые он периодически получает, и сигналами, которые он периодически посылает другим процессорам.
Т.о. нейронная сеть представляет собой совокупность нейроподобных элементов, определенным образом связанных между собой и с внешней средой с помощью связей,
определяемых весовыми коэффициентами.
   
2.9. 
Можно выделить три типа ИН, в зависимости от их ф-ий в сети:
    - входные - служат лишь для распределения входных сигналов (распределяющие нейроны). Они не выполняют каких-либо вычислений, поэтому не считаются слоем. На схемах часто обозначаются кругами. 
    - выходные - выходы нейронной сети
    - помежуточные нейроны - основа нейронной сети
    
      входные             промежуточные
         |                      |
         V                      V
x1 -->   () |  ---w 1,1 --->   [K1] | ---
            |                       |
x2 -->   () |  ---w 1,2 --->   [K2] | ---
            |                       |
x1 -->   () |  ---w 1,3 --->  [K3]  | ---
            |                       |
            >  один  слой нейронов  < 

Очень обобщенные выводы:

Вычислительные системы, основанные на ИНС, имеют ряд качеств, которые отсутствуют в машинах с архитектурой фон Неймана ( присущи мозгу человека ):

    - обучаемость;
    - способность к обобщению;
    - ассоциативное, распределенное хранение данных;
    - адаптация к изменению окружающей среды;
    - потенциально высокая производительность;
    - отказоустойчивость аппаратной реализации.    
  ********  
3. Обучение  
3.1 Алгорит обучения - процедура, используемая для процесса обучения.
    Эта процедура выстраивает в определенном порядке синаптические веса нейронной сети для обеспечения необходимой структуры взаимосвязей нейронов.
    (Заметка: Изменение синаптических весов представляет собой традиционный метод настройки нейронных сетей. Этот подход очень близок к теории линейных адаптивных фильтров, которая применяется
    в различных областях деятельности человека.)

